{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaoyi/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary package\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a naive multilayer perceptron\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  \n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3*32*32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,10) # the second value must be 10 because cifar10 dataset has 10 classes.\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(self.flatten(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Prepare CIFAR-10 dataset\n",
    "trainset = CIFAR10(root='cifar10', train=\"True\", download=True, transform=transforms.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=1)\n",
    "testset = CIFAR10(root='cifar10', train=\"False\", download=True, transform=transforms.ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OTO graph constructor\n",
      "graph build\n"
     ]
    }
   ],
   "source": [
    "# Create OTO instance\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from only_train_once import OTO\n",
    "\n",
    "model = MLP() # Instantiate the model\n",
    "dummy_input = torch.rand(1, 3, 32, 32)\n",
    "oto = OTO(model=model.cuda(), dummy_input=dummy_input.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup HESSO\n",
      "Target redundant groups per period:  [4, 4, 4, 4, 4, 4, 4, 4, 4, 12]\n"
     ]
    }
   ],
   "source": [
    "# Set up the Hesso optimizer\n",
    "optimizer = oto.hesso(\n",
    "    variant='sgd', \n",
    "    lr=0.1, \n",
    "    weight_decay=1e-4,\n",
    "    target_group_sparsity=0.5,\n",
    "    start_pruning_step=10 * len(trainloader), \n",
    "    pruning_periods=10,\n",
    "    pruning_steps=10 * len(trainloader)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaoyi/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 0, loss: 1.97, norm_all:68.85, grp_sparsity: 0.00, acc1: 0.2846, norm_import: 68.85, norm_redund: 0.00, num_grp_import: 96, num_grp_redund: 0\n",
      "Ep: 1, loss: 1.78, norm_all:79.18, grp_sparsity: 0.00, acc1: 0.2817, norm_import: 79.18, norm_redund: 0.00, num_grp_import: 96, num_grp_redund: 0\n",
      "Ep: 2, loss: 1.70, norm_all:88.29, grp_sparsity: 0.00, acc1: 0.3699, norm_import: 88.29, norm_redund: 0.00, num_grp_import: 96, num_grp_redund: 0\n",
      "Ep: 3, loss: 1.66, norm_all:96.56, grp_sparsity: 0.00, acc1: 0.3959, norm_import: 96.56, norm_redund: 0.00, num_grp_import: 96, num_grp_redund: 0\n",
      "Ep: 4, loss: 1.61, norm_all:103.96, grp_sparsity: 0.00, acc1: 0.3466, norm_import: 103.96, norm_redund: 0.00, num_grp_import: 96, num_grp_redund: 0\n",
      "Ep: 5, loss: 1.59, norm_all:110.85, grp_sparsity: 0.00, acc1: 0.4124, norm_import: 110.85, norm_redund: 0.00, num_grp_import: 96, num_grp_redund: 0\n",
      "Ep: 6, loss: 1.56, norm_all:117.09, grp_sparsity: 0.00, acc1: 0.3604, norm_import: 117.09, norm_redund: 0.00, num_grp_import: 96, num_grp_redund: 0\n",
      "Ep: 7, loss: 1.53, norm_all:122.80, grp_sparsity: 0.00, acc1: 0.4691, norm_import: 122.80, norm_redund: 0.00, num_grp_import: 96, num_grp_redund: 0\n",
      "Ep: 8, loss: 1.52, norm_all:128.23, grp_sparsity: 0.00, acc1: 0.4359, norm_import: 128.23, norm_redund: 0.00, num_grp_import: 96, num_grp_redund: 0\n",
      "Ep: 9, loss: 1.50, norm_all:133.39, grp_sparsity: 0.00, acc1: 0.4242, norm_import: 133.39, norm_redund: 0.00, num_grp_import: 96, num_grp_redund: 0\n",
      "0 ['linear_relu_stack.2.weight', 'linear_relu_stack.2.bias'] torch.Size([32])\n",
      "1 ['linear_relu_stack.0.weight', 'linear_relu_stack.0.bias'] torch.Size([64])\n",
      "Ep: 10, loss: 1.49, norm_all:135.96, grp_sparsity: 0.04, acc1: 0.3523, norm_import: 135.96, norm_redund: 0.00, num_grp_import: 92, num_grp_redund: 4\n",
      "0 ['linear_relu_stack.2.weight', 'linear_relu_stack.2.bias'] torch.Size([32])\n",
      "1 ['linear_relu_stack.0.weight', 'linear_relu_stack.0.bias'] torch.Size([64])\n",
      "Ep: 11, loss: 1.48, norm_all:137.85, grp_sparsity: 0.08, acc1: 0.4224, norm_import: 137.85, norm_redund: 0.00, num_grp_import: 88, num_grp_redund: 8\n",
      "0 ['linear_relu_stack.2.weight', 'linear_relu_stack.2.bias'] torch.Size([32])\n",
      "1 ['linear_relu_stack.0.weight', 'linear_relu_stack.0.bias'] torch.Size([64])\n",
      "Ep: 12, loss: 1.47, norm_all:138.49, grp_sparsity: 0.12, acc1: 0.3991, norm_import: 138.49, norm_redund: 0.00, num_grp_import: 84, num_grp_redund: 12\n",
      "0 ['linear_relu_stack.2.weight', 'linear_relu_stack.2.bias'] torch.Size([32])\n",
      "1 ['linear_relu_stack.0.weight', 'linear_relu_stack.0.bias'] torch.Size([64])\n",
      "Ep: 13, loss: 1.46, norm_all:138.17, grp_sparsity: 0.17, acc1: 0.4281, norm_import: 138.17, norm_redund: 0.00, num_grp_import: 80, num_grp_redund: 16\n",
      "0 ['linear_relu_stack.2.weight', 'linear_relu_stack.2.bias'] torch.Size([32])\n",
      "1 ['linear_relu_stack.0.weight', 'linear_relu_stack.0.bias'] torch.Size([64])\n",
      "Ep: 14, loss: 1.45, norm_all:138.68, grp_sparsity: 0.21, acc1: 0.4155, norm_import: 138.68, norm_redund: 0.00, num_grp_import: 76, num_grp_redund: 20\n",
      "0 ['linear_relu_stack.2.weight', 'linear_relu_stack.2.bias'] torch.Size([32])\n",
      "1 ['linear_relu_stack.0.weight', 'linear_relu_stack.0.bias'] torch.Size([64])\n",
      "Ep: 15, loss: 1.45, norm_all:138.94, grp_sparsity: 0.25, acc1: 0.4489, norm_import: 138.94, norm_redund: 0.00, num_grp_import: 72, num_grp_redund: 24\n",
      "0 ['linear_relu_stack.2.weight', 'linear_relu_stack.2.bias'] torch.Size([32])\n",
      "1 ['linear_relu_stack.0.weight', 'linear_relu_stack.0.bias'] torch.Size([64])\n",
      "Ep: 16, loss: 1.43, norm_all:140.12, grp_sparsity: 0.29, acc1: 0.4984, norm_import: 140.12, norm_redund: 0.00, num_grp_import: 68, num_grp_redund: 28\n",
      "0 ['linear_relu_stack.2.weight', 'linear_relu_stack.2.bias'] torch.Size([32])\n",
      "1 ['linear_relu_stack.0.weight', 'linear_relu_stack.0.bias'] torch.Size([64])\n",
      "Ep: 17, loss: 1.44, norm_all:137.98, grp_sparsity: 0.33, acc1: 0.4710, norm_import: 137.98, norm_redund: 0.00, num_grp_import: 64, num_grp_redund: 32\n",
      "0 ['linear_relu_stack.2.weight', 'linear_relu_stack.2.bias'] torch.Size([32])\n",
      "1 ['linear_relu_stack.0.weight', 'linear_relu_stack.0.bias'] torch.Size([64])\n",
      "Ep: 18, loss: 1.45, norm_all:131.51, grp_sparsity: 0.37, acc1: 0.4664, norm_import: 131.51, norm_redund: 0.00, num_grp_import: 60, num_grp_redund: 36\n",
      "0 ['linear_relu_stack.2.weight', 'linear_relu_stack.2.bias'] torch.Size([32])\n",
      "1 ['linear_relu_stack.0.weight', 'linear_relu_stack.0.bias'] torch.Size([64])\n",
      "Ep: 19, loss: 1.52, norm_all:102.30, grp_sparsity: 0.50, acc1: 0.3133, norm_import: 102.30, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 20, loss: 1.55, norm_all:104.70, grp_sparsity: 0.50, acc1: 0.4365, norm_import: 104.70, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 21, loss: 1.53, norm_all:106.77, grp_sparsity: 0.50, acc1: 0.4320, norm_import: 106.77, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 22, loss: 1.52, norm_all:108.87, grp_sparsity: 0.50, acc1: 0.3817, norm_import: 108.87, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 23, loss: 1.51, norm_all:110.75, grp_sparsity: 0.50, acc1: 0.4376, norm_import: 110.75, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 24, loss: 1.50, norm_all:112.59, grp_sparsity: 0.50, acc1: 0.3383, norm_import: 112.59, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 25, loss: 1.50, norm_all:114.19, grp_sparsity: 0.50, acc1: 0.3720, norm_import: 114.19, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 26, loss: 1.49, norm_all:115.93, grp_sparsity: 0.50, acc1: 0.4274, norm_import: 115.93, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 27, loss: 1.48, norm_all:117.54, grp_sparsity: 0.50, acc1: 0.3965, norm_import: 117.54, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 28, loss: 1.48, norm_all:118.97, grp_sparsity: 0.50, acc1: 0.4494, norm_import: 118.97, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 29, loss: 1.47, norm_all:120.46, grp_sparsity: 0.50, acc1: 0.4406, norm_import: 120.46, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 30, loss: 1.47, norm_all:121.74, grp_sparsity: 0.50, acc1: 0.4523, norm_import: 121.74, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 31, loss: 1.47, norm_all:123.15, grp_sparsity: 0.50, acc1: 0.3697, norm_import: 123.15, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 32, loss: 1.46, norm_all:124.48, grp_sparsity: 0.50, acc1: 0.4199, norm_import: 124.48, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 33, loss: 1.46, norm_all:125.76, grp_sparsity: 0.50, acc1: 0.4327, norm_import: 125.76, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 34, loss: 1.46, norm_all:126.87, grp_sparsity: 0.50, acc1: 0.3907, norm_import: 126.87, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 35, loss: 1.46, norm_all:128.20, grp_sparsity: 0.50, acc1: 0.3932, norm_import: 128.20, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 36, loss: 1.45, norm_all:129.26, grp_sparsity: 0.50, acc1: 0.4417, norm_import: 129.26, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 37, loss: 1.45, norm_all:130.25, grp_sparsity: 0.50, acc1: 0.4646, norm_import: 130.25, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 38, loss: 1.45, norm_all:131.33, grp_sparsity: 0.50, acc1: 0.3910, norm_import: 131.33, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 39, loss: 1.45, norm_all:132.36, grp_sparsity: 0.50, acc1: 0.4328, norm_import: 132.36, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 40, loss: 1.44, norm_all:133.32, grp_sparsity: 0.50, acc1: 0.4158, norm_import: 133.32, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 41, loss: 1.44, norm_all:134.45, grp_sparsity: 0.50, acc1: 0.4459, norm_import: 134.45, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 42, loss: 1.44, norm_all:135.50, grp_sparsity: 0.50, acc1: 0.4416, norm_import: 135.50, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 43, loss: 1.44, norm_all:136.45, grp_sparsity: 0.50, acc1: 0.4450, norm_import: 136.45, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 44, loss: 1.44, norm_all:137.33, grp_sparsity: 0.50, acc1: 0.4486, norm_import: 137.33, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 45, loss: 1.43, norm_all:138.28, grp_sparsity: 0.50, acc1: 0.4440, norm_import: 138.28, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 46, loss: 1.43, norm_all:139.09, grp_sparsity: 0.50, acc1: 0.4331, norm_import: 139.09, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 47, loss: 1.43, norm_all:140.07, grp_sparsity: 0.50, acc1: 0.4229, norm_import: 140.07, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 48, loss: 1.43, norm_all:140.86, grp_sparsity: 0.50, acc1: 0.4478, norm_import: 140.86, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 49, loss: 1.32, norm_all:140.94, grp_sparsity: 0.50, acc1: 0.5297, norm_import: 140.94, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 50, loss: 1.31, norm_all:140.95, grp_sparsity: 0.50, acc1: 0.5308, norm_import: 140.95, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 51, loss: 1.31, norm_all:140.94, grp_sparsity: 0.50, acc1: 0.5315, norm_import: 140.94, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 52, loss: 1.31, norm_all:140.92, grp_sparsity: 0.50, acc1: 0.5317, norm_import: 140.92, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 53, loss: 1.31, norm_all:140.89, grp_sparsity: 0.50, acc1: 0.5313, norm_import: 140.89, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 54, loss: 1.30, norm_all:140.86, grp_sparsity: 0.50, acc1: 0.5365, norm_import: 140.86, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 55, loss: 1.30, norm_all:140.83, grp_sparsity: 0.50, acc1: 0.5353, norm_import: 140.83, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 56, loss: 1.30, norm_all:140.80, grp_sparsity: 0.50, acc1: 0.5311, norm_import: 140.80, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 57, loss: 1.30, norm_all:140.76, grp_sparsity: 0.50, acc1: 0.5367, norm_import: 140.76, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 58, loss: 1.30, norm_all:140.73, grp_sparsity: 0.50, acc1: 0.5282, norm_import: 140.73, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 59, loss: 1.30, norm_all:140.70, grp_sparsity: 0.50, acc1: 0.5357, norm_import: 140.70, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 60, loss: 1.30, norm_all:140.66, grp_sparsity: 0.50, acc1: 0.5340, norm_import: 140.66, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 61, loss: 1.30, norm_all:140.62, grp_sparsity: 0.50, acc1: 0.5353, norm_import: 140.62, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 62, loss: 1.29, norm_all:140.61, grp_sparsity: 0.50, acc1: 0.5357, norm_import: 140.61, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 63, loss: 1.30, norm_all:140.58, grp_sparsity: 0.50, acc1: 0.5383, norm_import: 140.58, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 64, loss: 1.29, norm_all:140.54, grp_sparsity: 0.50, acc1: 0.5363, norm_import: 140.54, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 65, loss: 1.29, norm_all:140.51, grp_sparsity: 0.50, acc1: 0.5348, norm_import: 140.51, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 66, loss: 1.29, norm_all:140.48, grp_sparsity: 0.50, acc1: 0.5368, norm_import: 140.48, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 67, loss: 1.29, norm_all:140.43, grp_sparsity: 0.50, acc1: 0.5376, norm_import: 140.43, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 68, loss: 1.29, norm_all:140.40, grp_sparsity: 0.50, acc1: 0.5419, norm_import: 140.40, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 69, loss: 1.29, norm_all:140.37, grp_sparsity: 0.50, acc1: 0.5371, norm_import: 140.37, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 70, loss: 1.29, norm_all:140.34, grp_sparsity: 0.50, acc1: 0.5378, norm_import: 140.34, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 71, loss: 1.29, norm_all:140.30, grp_sparsity: 0.50, acc1: 0.5345, norm_import: 140.30, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 72, loss: 1.29, norm_all:140.26, grp_sparsity: 0.50, acc1: 0.5406, norm_import: 140.26, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 73, loss: 1.29, norm_all:140.24, grp_sparsity: 0.50, acc1: 0.5400, norm_import: 140.24, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 74, loss: 1.29, norm_all:140.20, grp_sparsity: 0.50, acc1: 0.5395, norm_import: 140.20, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 75, loss: 1.29, norm_all:140.17, grp_sparsity: 0.50, acc1: 0.5397, norm_import: 140.17, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 76, loss: 1.28, norm_all:140.14, grp_sparsity: 0.50, acc1: 0.5411, norm_import: 140.14, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 77, loss: 1.28, norm_all:140.12, grp_sparsity: 0.50, acc1: 0.5407, norm_import: 140.12, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 78, loss: 1.28, norm_all:140.09, grp_sparsity: 0.50, acc1: 0.5399, norm_import: 140.09, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 79, loss: 1.28, norm_all:140.06, grp_sparsity: 0.50, acc1: 0.5385, norm_import: 140.06, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 80, loss: 1.28, norm_all:140.03, grp_sparsity: 0.50, acc1: 0.5405, norm_import: 140.03, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 81, loss: 1.28, norm_all:140.00, grp_sparsity: 0.50, acc1: 0.5404, norm_import: 140.00, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 82, loss: 1.28, norm_all:139.97, grp_sparsity: 0.50, acc1: 0.5420, norm_import: 139.97, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 83, loss: 1.28, norm_all:139.94, grp_sparsity: 0.50, acc1: 0.5427, norm_import: 139.94, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 84, loss: 1.28, norm_all:139.91, grp_sparsity: 0.50, acc1: 0.5418, norm_import: 139.91, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 85, loss: 1.28, norm_all:139.88, grp_sparsity: 0.50, acc1: 0.5421, norm_import: 139.88, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 86, loss: 1.28, norm_all:139.86, grp_sparsity: 0.50, acc1: 0.5419, norm_import: 139.86, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 87, loss: 1.28, norm_all:139.83, grp_sparsity: 0.50, acc1: 0.5449, norm_import: 139.83, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 88, loss: 1.28, norm_all:139.80, grp_sparsity: 0.50, acc1: 0.5440, norm_import: 139.80, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 89, loss: 1.28, norm_all:139.78, grp_sparsity: 0.50, acc1: 0.5440, norm_import: 139.78, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 90, loss: 1.28, norm_all:139.75, grp_sparsity: 0.50, acc1: 0.5458, norm_import: 139.75, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 91, loss: 1.28, norm_all:139.72, grp_sparsity: 0.50, acc1: 0.5435, norm_import: 139.72, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 92, loss: 1.28, norm_all:139.69, grp_sparsity: 0.50, acc1: 0.5438, norm_import: 139.69, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 93, loss: 1.28, norm_all:139.66, grp_sparsity: 0.50, acc1: 0.5434, norm_import: 139.66, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 94, loss: 1.28, norm_all:139.63, grp_sparsity: 0.50, acc1: 0.5406, norm_import: 139.63, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 95, loss: 1.27, norm_all:139.61, grp_sparsity: 0.50, acc1: 0.5420, norm_import: 139.61, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 96, loss: 1.27, norm_all:139.59, grp_sparsity: 0.50, acc1: 0.5445, norm_import: 139.59, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 97, loss: 1.27, norm_all:139.57, grp_sparsity: 0.50, acc1: 0.5456, norm_import: 139.57, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 98, loss: 1.27, norm_all:139.55, grp_sparsity: 0.50, acc1: 0.5449, norm_import: 139.55, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 99, loss: 1.26, norm_all:139.55, grp_sparsity: 0.50, acc1: 0.5503, norm_import: 139.55, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import check_accuracy\n",
    "\n",
    "max_epoch = 100\n",
    "model.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Every 50 epochs, decay lr by 10.0\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1) \n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    f_avg_val = 0.0\n",
    "    lr_scheduler.step()\n",
    "    for X, y in trainloader:\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "        y_pred = model.forward(X)\n",
    "        f = criterion(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        f.backward()\n",
    "        f_avg_val += f\n",
    "        optimizer.step()\n",
    "    opt_metrics = optimizer.compute_metrics()\n",
    "    \n",
    "    accuracy1, accuracy5 = check_accuracy(model, testloader)\n",
    "    # accuracy1, accuracy5 = check_accuracy(model, trainloader)\n",
    "    f_avg_val = f_avg_val.cpu().item() / len(trainloader)\n",
    "    \n",
    "    print(\"Ep: {ep}, loss: {f:.2f}, norm_all:{param_norm:.2f}, grp_sparsity: {gs:.2f}, acc1: {acc1:.4f}, norm_import: {norm_import:.2f}, norm_redund: {norm_redund:.2f}, num_grp_import: {num_grps_import}, num_grp_redund: {num_grps_redund}\"\\\n",
    "         .format(ep=epoch, f=f_avg_val, param_norm=opt_metrics.norm_params, gs=opt_metrics.group_sparsity, acc1=accuracy1,\\\n",
    "         norm_import=opt_metrics.norm_important_groups, norm_redund=opt_metrics.norm_redundant_groups, \\\n",
    "         num_grps_import=opt_metrics.num_important_groups, num_grps_redund=opt_metrics.num_redundant_groups\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of full model     :  0.000745970755815506 GBs\n",
      "Size of compress model :  0.00021345727145671844 GBs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model: Acc 1: 0.55028, Acc 5: 0.94488\n",
      "Compressed model: Acc 1: 0.55028, Acc 5: 0.94488\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "oto.construct_subnet(out_dir='./cache')\n",
    "\n",
    "# Compare the full model size and compressed model size\n",
    "full_model_size = os.stat(oto.full_group_sparse_model_path)\n",
    "compressed_model_size = os.stat(oto.compressed_model_path)\n",
    "print(\"Size of full model     : \", full_model_size.st_size / (1024 ** 3), \"GBs\")\n",
    "print(\"Size of compress model : \", compressed_model_size.st_size / (1024 ** 3), \"GBs\")\n",
    "\n",
    "# Both full and compressed model should return the exact same accuracy.\n",
    "full_model = torch.load(oto.full_group_sparse_model_path)\n",
    "compressed_model = torch.load(oto.compressed_model_path)\n",
    "\n",
    "acc1_full, acc5_full = check_accuracy(full_model, testloader)\n",
    "print(\"Full model: Acc 1: {acc1}, Acc 5: {acc5}\".format(acc1=acc1_full, acc5=acc5_full))\n",
    "\n",
    "acc1_compressed, acc5_compressed = check_accuracy(compressed_model, testloader)\n",
    "print(\"Compressed model: Acc 1: {acc1}, Acc 5: {acc5}\".format(acc1=acc1_compressed, acc5=acc5_compressed))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
