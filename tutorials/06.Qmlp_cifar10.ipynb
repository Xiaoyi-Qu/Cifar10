{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OTO graph constructor\n",
      "graph build\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaoyi/otov2/otov2_auto_structured_pruning/tutorials/../sanity_check/backends/qnn.py:67: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  weight_clip_val = torch.tensor([-200.0, 200.0])\n",
      "/home/xiaoyi/otov2/otov2_auto_structured_pruning/tutorials/../sanity_check/backends/qnn.py:72: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  act_clip_val = torch.tensor([-200.0, 200.0])\n"
     ]
    }
   ],
   "source": [
    "# Create an OTO instance\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from sanity_check.backends.qnn import QMLP\n",
    "from only_train_once import OTO\n",
    "    \n",
    "model = QMLP(3*32*32, 64, 32, 10) # Instantiate the model\n",
    "dummy_input = torch.rand(1, 3, 32, 32)\n",
    "oto = OTO(model=model.cuda(), dummy_input=dummy_input.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Prepare CIFAR-10 dataset\n",
    "trainset = CIFAR10(root='cifar10', train=\"True\", download=True, transform=transforms.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=1)\n",
    "testset = CIFAR10(root='cifar10', train=\"False\", download=True, transform=transforms.ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the independency graph (optional)\n",
    "oto.visualize(view=False, out_dir='./cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup HESSO\n",
      "Target redundant groups per period:  [4, 4, 4, 4, 4, 4, 4, 4, 4, 12]\n"
     ]
    }
   ],
   "source": [
    "# Set up the Hesso optimizer\n",
    "optimizer = oto.hesso(\n",
    "    variant='sgd', \n",
    "    lr=0.1, \n",
    "    weight_decay=1e-4,\n",
    "    target_group_sparsity=0.5,\n",
    "    start_pruning_step=10 * len(trainloader), \n",
    "    pruning_periods=10,\n",
    "    pruning_steps=10 * len(trainloader)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaoyi/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 0, loss: 1.96, norm_all:70.26, grp_sparsity: 0.00, acc1: 0.3179, norm_import: 70.26, norm_redund: 0.00, num_grp_import: 96, num_grp_redund: 0\n",
      "Ep: 1, loss: 1.77, norm_all:81.14, grp_sparsity: 0.00, acc1: 0.3523, norm_import: 81.14, norm_redund: 0.00, num_grp_import: 96, num_grp_redund: 0\n",
      "Ep: 2, loss: 1.70, norm_all:90.92, grp_sparsity: 0.00, acc1: 0.3343, norm_import: 90.92, norm_redund: 0.00, num_grp_import: 96, num_grp_redund: 0\n",
      "Ep: 3, loss: 1.65, norm_all:99.66, grp_sparsity: 0.00, acc1: 0.3892, norm_import: 99.66, norm_redund: 0.00, num_grp_import: 96, num_grp_redund: 0\n",
      "Ep: 4, loss: 1.61, norm_all:107.20, grp_sparsity: 0.00, acc1: 0.3937, norm_import: 107.20, norm_redund: 0.00, num_grp_import: 96, num_grp_redund: 0\n",
      "Ep: 5, loss: 1.58, norm_all:114.24, grp_sparsity: 0.00, acc1: 0.3852, norm_import: 114.24, norm_redund: 0.00, num_grp_import: 96, num_grp_redund: 0\n",
      "Ep: 6, loss: 1.55, norm_all:120.62, grp_sparsity: 0.00, acc1: 0.3990, norm_import: 120.62, norm_redund: 0.00, num_grp_import: 96, num_grp_redund: 0\n",
      "Ep: 7, loss: 1.53, norm_all:126.27, grp_sparsity: 0.00, acc1: 0.4189, norm_import: 126.27, norm_redund: 0.00, num_grp_import: 96, num_grp_redund: 0\n",
      "Ep: 8, loss: 1.51, norm_all:131.93, grp_sparsity: 0.00, acc1: 0.3933, norm_import: 131.93, norm_redund: 0.00, num_grp_import: 96, num_grp_redund: 0\n",
      "Ep: 9, loss: 1.49, norm_all:137.41, grp_sparsity: 0.00, acc1: 0.3776, norm_import: 137.41, norm_redund: 0.00, num_grp_import: 96, num_grp_redund: 0\n",
      "0 ['linear1.bias', 'linear1.weight'] torch.Size([64])\n",
      "1 ['linear2.weight', 'linear2.bias'] torch.Size([32])\n",
      "Ep: 10, loss: 1.48, norm_all:139.92, grp_sparsity: 0.04, acc1: 0.4458, norm_import: 139.92, norm_redund: 0.00, num_grp_import: 92, num_grp_redund: 4\n",
      "0 ['linear1.bias', 'linear1.weight'] torch.Size([64])\n",
      "1 ['linear2.weight', 'linear2.bias'] torch.Size([32])\n",
      "Ep: 11, loss: 1.47, norm_all:141.48, grp_sparsity: 0.08, acc1: 0.3839, norm_import: 141.48, norm_redund: 0.00, num_grp_import: 88, num_grp_redund: 8\n",
      "0 ['linear1.bias', 'linear1.weight'] torch.Size([64])\n",
      "1 ['linear2.weight', 'linear2.bias'] torch.Size([32])\n",
      "Ep: 12, loss: 1.47, norm_all:140.52, grp_sparsity: 0.12, acc1: 0.4430, norm_import: 140.52, norm_redund: 0.00, num_grp_import: 84, num_grp_redund: 12\n",
      "0 ['linear1.bias', 'linear1.weight'] torch.Size([64])\n",
      "1 ['linear2.weight', 'linear2.bias'] torch.Size([32])\n",
      "Ep: 13, loss: 1.46, norm_all:141.45, grp_sparsity: 0.17, acc1: 0.4125, norm_import: 141.45, norm_redund: 0.00, num_grp_import: 80, num_grp_redund: 16\n",
      "0 ['linear1.bias', 'linear1.weight'] torch.Size([64])\n",
      "1 ['linear2.weight', 'linear2.bias'] torch.Size([32])\n",
      "Ep: 14, loss: 1.45, norm_all:143.30, grp_sparsity: 0.21, acc1: 0.3985, norm_import: 143.30, norm_redund: 0.00, num_grp_import: 76, num_grp_redund: 20\n",
      "0 ['linear1.bias', 'linear1.weight'] torch.Size([64])\n",
      "1 ['linear2.weight', 'linear2.bias'] torch.Size([32])\n",
      "Ep: 15, loss: 1.44, norm_all:145.08, grp_sparsity: 0.25, acc1: 0.4374, norm_import: 145.08, norm_redund: 0.00, num_grp_import: 72, num_grp_redund: 24\n",
      "0 ['linear1.bias', 'linear1.weight'] torch.Size([64])\n",
      "1 ['linear2.weight', 'linear2.bias'] torch.Size([32])\n",
      "Ep: 16, loss: 1.44, norm_all:144.84, grp_sparsity: 0.29, acc1: 0.4739, norm_import: 144.84, norm_redund: 0.00, num_grp_import: 68, num_grp_redund: 28\n",
      "0 ['linear1.bias', 'linear1.weight'] torch.Size([64])\n",
      "1 ['linear2.weight', 'linear2.bias'] torch.Size([32])\n",
      "Ep: 17, loss: 1.44, norm_all:139.78, grp_sparsity: 0.33, acc1: 0.4455, norm_import: 139.78, norm_redund: 0.00, num_grp_import: 64, num_grp_redund: 32\n",
      "0 ['linear1.bias', 'linear1.weight'] torch.Size([64])\n",
      "1 ['linear2.weight', 'linear2.bias'] torch.Size([32])\n",
      "Ep: 18, loss: 1.45, norm_all:132.37, grp_sparsity: 0.37, acc1: 0.4509, norm_import: 132.37, norm_redund: 0.00, num_grp_import: 60, num_grp_redund: 36\n",
      "0 ['linear1.bias', 'linear1.weight'] torch.Size([64])\n",
      "1 ['linear2.weight', 'linear2.bias'] torch.Size([32])\n",
      "Ep: 19, loss: 1.51, norm_all:106.03, grp_sparsity: 0.50, acc1: 0.3859, norm_import: 106.03, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 20, loss: 1.54, norm_all:108.65, grp_sparsity: 0.50, acc1: 0.3490, norm_import: 108.65, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 21, loss: 1.52, norm_all:110.97, grp_sparsity: 0.50, acc1: 0.3942, norm_import: 110.97, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 22, loss: 1.51, norm_all:113.06, grp_sparsity: 0.50, acc1: 0.4231, norm_import: 113.06, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 23, loss: 1.50, norm_all:115.07, grp_sparsity: 0.50, acc1: 0.3970, norm_import: 115.07, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 24, loss: 1.50, norm_all:117.07, grp_sparsity: 0.50, acc1: 0.4717, norm_import: 117.07, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 25, loss: 1.49, norm_all:119.04, grp_sparsity: 0.50, acc1: 0.4051, norm_import: 119.04, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 26, loss: 1.48, norm_all:120.83, grp_sparsity: 0.50, acc1: 0.3849, norm_import: 120.83, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 27, loss: 1.48, norm_all:122.56, grp_sparsity: 0.50, acc1: 0.4613, norm_import: 122.56, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 28, loss: 1.47, norm_all:124.29, grp_sparsity: 0.50, acc1: 0.4359, norm_import: 124.29, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 29, loss: 1.47, norm_all:125.94, grp_sparsity: 0.50, acc1: 0.3633, norm_import: 125.94, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 30, loss: 1.46, norm_all:127.44, grp_sparsity: 0.50, acc1: 0.4368, norm_import: 127.44, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 31, loss: 1.46, norm_all:128.91, grp_sparsity: 0.50, acc1: 0.4519, norm_import: 128.91, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 32, loss: 1.46, norm_all:130.29, grp_sparsity: 0.50, acc1: 0.4563, norm_import: 130.29, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 33, loss: 1.45, norm_all:131.76, grp_sparsity: 0.50, acc1: 0.4547, norm_import: 131.76, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 34, loss: 1.45, norm_all:133.26, grp_sparsity: 0.50, acc1: 0.3962, norm_import: 133.26, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 35, loss: 1.45, norm_all:134.58, grp_sparsity: 0.50, acc1: 0.4271, norm_import: 134.58, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 36, loss: 1.44, norm_all:135.86, grp_sparsity: 0.50, acc1: 0.3891, norm_import: 135.86, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 37, loss: 1.44, norm_all:137.02, grp_sparsity: 0.50, acc1: 0.4177, norm_import: 137.02, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 38, loss: 1.44, norm_all:138.30, grp_sparsity: 0.50, acc1: 0.4767, norm_import: 138.30, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 39, loss: 1.43, norm_all:139.50, grp_sparsity: 0.50, acc1: 0.3856, norm_import: 139.50, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 40, loss: 1.43, norm_all:140.54, grp_sparsity: 0.50, acc1: 0.4470, norm_import: 140.54, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 41, loss: 1.43, norm_all:141.64, grp_sparsity: 0.50, acc1: 0.4270, norm_import: 141.64, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 42, loss: 1.43, norm_all:142.89, grp_sparsity: 0.50, acc1: 0.4429, norm_import: 142.89, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 43, loss: 1.43, norm_all:143.96, grp_sparsity: 0.50, acc1: 0.2909, norm_import: 143.96, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 44, loss: 1.42, norm_all:144.97, grp_sparsity: 0.50, acc1: 0.4783, norm_import: 144.97, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 45, loss: 1.42, norm_all:146.00, grp_sparsity: 0.50, acc1: 0.4563, norm_import: 146.00, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 46, loss: 1.42, norm_all:146.99, grp_sparsity: 0.50, acc1: 0.4264, norm_import: 146.99, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 47, loss: 1.42, norm_all:147.94, grp_sparsity: 0.50, acc1: 0.4384, norm_import: 147.94, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 48, loss: 1.42, norm_all:149.02, grp_sparsity: 0.50, acc1: 0.4086, norm_import: 149.02, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 49, loss: 1.31, norm_all:149.02, grp_sparsity: 0.50, acc1: 0.5329, norm_import: 149.02, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 50, loss: 1.30, norm_all:149.02, grp_sparsity: 0.50, acc1: 0.5361, norm_import: 149.02, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 51, loss: 1.29, norm_all:149.01, grp_sparsity: 0.50, acc1: 0.5361, norm_import: 149.01, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 52, loss: 1.29, norm_all:148.99, grp_sparsity: 0.50, acc1: 0.5368, norm_import: 148.99, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 53, loss: 1.29, norm_all:148.96, grp_sparsity: 0.50, acc1: 0.5354, norm_import: 148.96, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 54, loss: 1.29, norm_all:148.94, grp_sparsity: 0.50, acc1: 0.5364, norm_import: 148.94, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 55, loss: 1.29, norm_all:148.91, grp_sparsity: 0.50, acc1: 0.5348, norm_import: 148.91, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 56, loss: 1.28, norm_all:148.89, grp_sparsity: 0.50, acc1: 0.5395, norm_import: 148.89, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 57, loss: 1.28, norm_all:148.86, grp_sparsity: 0.50, acc1: 0.5408, norm_import: 148.86, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 58, loss: 1.28, norm_all:148.83, grp_sparsity: 0.50, acc1: 0.5384, norm_import: 148.83, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 59, loss: 1.28, norm_all:148.80, grp_sparsity: 0.50, acc1: 0.5412, norm_import: 148.80, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 60, loss: 1.28, norm_all:148.78, grp_sparsity: 0.50, acc1: 0.5373, norm_import: 148.78, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 61, loss: 1.28, norm_all:148.75, grp_sparsity: 0.50, acc1: 0.5407, norm_import: 148.75, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 62, loss: 1.28, norm_all:148.72, grp_sparsity: 0.50, acc1: 0.5393, norm_import: 148.72, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 63, loss: 1.28, norm_all:148.68, grp_sparsity: 0.50, acc1: 0.5420, norm_import: 148.68, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 64, loss: 1.28, norm_all:148.64, grp_sparsity: 0.50, acc1: 0.5391, norm_import: 148.64, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 65, loss: 1.28, norm_all:148.60, grp_sparsity: 0.50, acc1: 0.5417, norm_import: 148.60, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 66, loss: 1.28, norm_all:148.59, grp_sparsity: 0.50, acc1: 0.5385, norm_import: 148.59, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 67, loss: 1.27, norm_all:148.57, grp_sparsity: 0.50, acc1: 0.5383, norm_import: 148.57, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 68, loss: 1.27, norm_all:148.52, grp_sparsity: 0.50, acc1: 0.5408, norm_import: 148.52, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 69, loss: 1.27, norm_all:148.47, grp_sparsity: 0.50, acc1: 0.5433, norm_import: 148.47, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 70, loss: 1.27, norm_all:148.45, grp_sparsity: 0.50, acc1: 0.5406, norm_import: 148.45, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 71, loss: 1.27, norm_all:148.42, grp_sparsity: 0.50, acc1: 0.5431, norm_import: 148.42, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 72, loss: 1.27, norm_all:148.38, grp_sparsity: 0.50, acc1: 0.5418, norm_import: 148.38, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 73, loss: 1.27, norm_all:148.36, grp_sparsity: 0.50, acc1: 0.5441, norm_import: 148.36, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 74, loss: 1.27, norm_all:148.32, grp_sparsity: 0.50, acc1: 0.5427, norm_import: 148.32, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 75, loss: 1.27, norm_all:148.28, grp_sparsity: 0.50, acc1: 0.5431, norm_import: 148.28, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 76, loss: 1.27, norm_all:148.26, grp_sparsity: 0.50, acc1: 0.5460, norm_import: 148.26, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 77, loss: 1.27, norm_all:148.23, grp_sparsity: 0.50, acc1: 0.5469, norm_import: 148.23, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 78, loss: 1.27, norm_all:148.19, grp_sparsity: 0.50, acc1: 0.5420, norm_import: 148.19, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 79, loss: 1.27, norm_all:148.16, grp_sparsity: 0.50, acc1: 0.5420, norm_import: 148.16, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 80, loss: 1.27, norm_all:148.13, grp_sparsity: 0.50, acc1: 0.5422, norm_import: 148.13, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 81, loss: 1.27, norm_all:148.09, grp_sparsity: 0.50, acc1: 0.5469, norm_import: 148.09, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 82, loss: 1.26, norm_all:148.08, grp_sparsity: 0.50, acc1: 0.5432, norm_import: 148.08, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 83, loss: 1.26, norm_all:148.04, grp_sparsity: 0.50, acc1: 0.5466, norm_import: 148.04, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 84, loss: 1.26, norm_all:148.01, grp_sparsity: 0.50, acc1: 0.5464, norm_import: 148.01, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 85, loss: 1.26, norm_all:147.96, grp_sparsity: 0.50, acc1: 0.5463, norm_import: 147.96, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 86, loss: 1.26, norm_all:147.95, grp_sparsity: 0.50, acc1: 0.5445, norm_import: 147.95, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 87, loss: 1.26, norm_all:147.92, grp_sparsity: 0.50, acc1: 0.5464, norm_import: 147.92, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 88, loss: 1.26, norm_all:147.88, grp_sparsity: 0.50, acc1: 0.5477, norm_import: 147.88, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 89, loss: 1.26, norm_all:147.84, grp_sparsity: 0.50, acc1: 0.5477, norm_import: 147.84, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 90, loss: 1.26, norm_all:147.81, grp_sparsity: 0.50, acc1: 0.5460, norm_import: 147.81, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 91, loss: 1.26, norm_all:147.79, grp_sparsity: 0.50, acc1: 0.5453, norm_import: 147.79, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 92, loss: 1.26, norm_all:147.77, grp_sparsity: 0.50, acc1: 0.5482, norm_import: 147.77, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 93, loss: 1.26, norm_all:147.75, grp_sparsity: 0.50, acc1: 0.5349, norm_import: 147.75, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 94, loss: 1.26, norm_all:147.72, grp_sparsity: 0.50, acc1: 0.5464, norm_import: 147.72, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 95, loss: 1.26, norm_all:147.68, grp_sparsity: 0.50, acc1: 0.5493, norm_import: 147.68, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 96, loss: 1.26, norm_all:147.65, grp_sparsity: 0.50, acc1: 0.5448, norm_import: 147.65, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 97, loss: 1.26, norm_all:147.63, grp_sparsity: 0.50, acc1: 0.5504, norm_import: 147.63, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 98, loss: 1.26, norm_all:147.60, grp_sparsity: 0.50, acc1: 0.5482, norm_import: 147.60, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n",
      "Ep: 99, loss: 1.24, norm_all:147.60, grp_sparsity: 0.50, acc1: 0.5539, norm_import: 147.60, norm_redund: 0.00, num_grp_import: 48, num_grp_redund: 48\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import check_accuracy\n",
    "\n",
    "max_epoch = 100\n",
    "model.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Every 50 epochs, decay lr by 10.0\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1) \n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    f_avg_val = 0.0\n",
    "    lr_scheduler.step()\n",
    "    for X, y in trainloader:\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "        y_pred = model.forward(X)\n",
    "        f = criterion(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        f.backward()\n",
    "        f_avg_val += f\n",
    "        optimizer.step()\n",
    "    opt_metrics = optimizer.compute_metrics()\n",
    "    \n",
    "    accuracy1, accuracy5 = check_accuracy(model, testloader)\n",
    "    # accuracy1, accuracy5 = check_accuracy(model, trainloader)\n",
    "    f_avg_val = f_avg_val.cpu().item() / len(trainloader)\n",
    "    \n",
    "    print(\"Ep: {ep}, loss: {f:.2f}, norm_all:{param_norm:.2f}, grp_sparsity: {gs:.2f}, acc1: {acc1:.4f}, norm_import: {norm_import:.2f}, norm_redund: {norm_redund:.2f}, num_grp_import: {num_grps_import}, num_grp_redund: {num_grps_redund}\"\\\n",
    "         .format(ep=epoch, f=f_avg_val, param_norm=opt_metrics.norm_params, gs=opt_metrics.group_sparsity, acc1=accuracy1,\\\n",
    "         norm_import=opt_metrics.norm_important_groups, norm_redund=opt_metrics.norm_redundant_groups, \\\n",
    "         num_grps_import=opt_metrics.num_important_groups, num_grps_redund=opt_metrics.num_redundant_groups\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get compressed model in torch format\n",
    "oto.construct_subnet(out_dir='./cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of full model     :  0.0007456224411725998 GBs\n",
      "Size of compress model :  0.00023599714040756226 GBs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model: Acc 1: 0.55392, Acc 5: 0.94594\n",
      "Compressed model: Acc 1: 0.55392, Acc 5: 0.94594\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Compare the full model size and compressed model size\n",
    "full_model_size = os.stat(oto.full_group_sparse_model_path)\n",
    "compressed_model_size = os.stat(oto.compressed_model_path)\n",
    "print(\"Size of full model     : \", full_model_size.st_size / (1024 ** 3), \"GBs\")\n",
    "print(\"Size of compress model : \", compressed_model_size.st_size / (1024 ** 3), \"GBs\")\n",
    "\n",
    "# Both full and compressed model should return the exact same accuracy.\n",
    "full_model = torch.load(oto.full_group_sparse_model_path)\n",
    "compressed_model = torch.load(oto.compressed_model_path)\n",
    "\n",
    "acc1_full, acc5_full = check_accuracy(full_model, testloader)\n",
    "print(\"Full model: Acc 1: {acc1}, Acc 5: {acc5}\".format(acc1=acc1_full, acc5=acc5_full))\n",
    "\n",
    "acc1_compressed, acc5_compressed = check_accuracy(compressed_model, testloader)\n",
    "print(\"Compressed model: Acc 1: {acc1}, Acc 5: {acc5}\".format(acc1=acc1_compressed, acc5=acc5_compressed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight\n",
      "\n",
      "\n",
      "64\n",
      "\n",
      "\n",
      "linear1.bias\n",
      "\n",
      "\n",
      "64\n",
      "\n",
      "\n",
      "linear2.weight\n",
      "\n",
      "\n",
      "32\n",
      "\n",
      "\n",
      "linear2.bias\n",
      "\n",
      "\n",
      "32\n",
      "\n",
      "\n",
      "linear3.weight\n",
      "\n",
      "\n",
      "10\n",
      "\n",
      "\n",
      "linear3.bias\n",
      "\n",
      "\n",
      "10\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add model compression block\n",
    "# how to store the weight?\n",
    "# how to store the activation value?\n",
    "# A useful github link: https://github.com/eladhoffer/quantized.pytorch/blob/master/models/resnet_quantized.py\n",
    "for n,p in full_model.state_dict().items():\n",
    "    print(n)\n",
    "    print(\"\\n\")\n",
    "    print(len(p))\n",
    "    print(\"\\n\")\n",
    "# def quantize_model(model):\n",
    "#     qparams = {}\n",
    "\n",
    "#     for n, p in model.state_dict().items():\n",
    "#         qp = quantize_tensor(p)\n",
    "#         qparams[n + '.quantization.scale'] = torch.FloatTensor([qp.scale])\n",
    "#         qparams[\n",
    "#             n + '.quantization.zero_point'] = torch.ByteTensor([qp.zero_point])\n",
    "#         p.copy_(qp.tensor)\n",
    "#     model.type('torch.ByteTensor')\n",
    "#     for n, p in qparams.items():\n",
    "#         model.register_buffer(n, p)\n",
    "#     model.quantized = True\n",
    "\n",
    "\n",
    "# def dequantize_model(model):\n",
    "#     model.float()\n",
    "#     params = model.state_dict()\n",
    "#     for n, p in params.items():\n",
    "#         if 'quantization' not in n:\n",
    "#             qp = QTensor(tensor=p,\n",
    "#                          scale=params[n + '.quantization.scale'][0],\n",
    "#                          zero_point=params[n + '.quantization.zero_point'][0])\n",
    "#             p.copy_(dequantize_tensor(qp))\n",
    "#             model.register_buffer(n + '.quantization.scale', None)\n",
    "#             model.register_buffer(n + '.quantization.zero_point', None)\n",
    "#     model.quantized = None\n",
    "\n",
    "# comments\n",
    "# (1) We have both activation and weights.\n",
    "# (2) Do we need to and how can we quantize weight. I checked one of the Github file. \n",
    "#     Not able to see the part that incorporates mdoel quantization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
